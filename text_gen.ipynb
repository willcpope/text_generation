{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text File to Train New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"lovecraft.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "\n",
    "Train a custom BPE Tokenizer on the downloaded text. This will save two files: aitextgen-vocab.json and aitextgen-merges.txt, which are needed to rebuild the tokenizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen.tokenizers:Saving aitextgen-vocab.json and aitextgen-merges.txt to the current directory. You will need both files to build the GPT2Tokenizer.\n"
     ]
    }
   ],
   "source": [
    "train_tokenizer(file_name)\n",
    "vocab_file = \"aitextgen-vocab.json\"\n",
    "merges_file = \"aitextgen-merges.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU Training\n",
    "GPT2ConfigCPU is a mini variant of GPT-2 optimized for CPU-training e.g. the # of input tokens here is 64 vs. 1024 for base GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = GPT2ConfigCPU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate aitextgen using the created tokenizer and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen:Constructing GPT-2 model from provided config.\n",
      "INFO:aitextgen:Using a custom tokenizer.\n"
     ]
    }
   ],
   "source": [
    "ai = aitextgen(vocab_file=vocab_file, \n",
    "               merges_file=merges_file, \n",
    "               config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset\n",
    "You can build datasets for training by creating TokenDatasets, which automatically processes the dataset with the appropriate size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20572f75216f46df99019ccf529188f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=8642.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen.TokenDataset:Encoding 8,642 sets of tokens from lovecraft.txt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = TokenDataset(file_name, \n",
    "                    vocab_file=vocab_file, \n",
    "                    merges_file=merges_file, \n",
    "                    block_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "It will save pytorch_model.bin periodically and after completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "INFO:lightning:GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8392bbadbd4ca38c06283917b4a677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=10000.0), HTML(value='')), layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m1,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"\n",
      "\n",
      "\n",
      "\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " I found, it be an I was the a more he was to the old be a house, and the the ancient of the night, and\n",
      "==========\n",
      "\u001b[1m2,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m2,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "-it to the very a more-lation. The he had was a womon, the way of the very dark, and the room was as he had seen, but had the time had been a small, the mited; and the cas. I had the great things in some it\n",
      "==========\n",
      "\u001b[1m3,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m3,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " it the time by that in those more rimo's thing, and the time.\n",
      "\n",
      "ONas.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Eeon'I knew that my day I were never told that he could only at the the Te the world, and the moment he's eyes--the hand\n",
      "==========\n",
      "\u001b[1m4,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m4,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ", and the old macole of Han, and I found the fanking, but had been been a great black and made him; and in the great-s were a moment of the town were a long, and the great-dreoator and the first nighted into the\n",
      "==========\n",
      "\u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m5,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ".\n",
      "\n",
      "The whole of the man in the city, who must have been a other than a ded in the dent, and now more than the place and were a way of his bal. He would be more than the first at a few which had been a moment. This one of course it\n",
      "==========\n",
      "\u001b[1m6,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m6,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "s and the mambles for the modousing out to the sea which he had to the sea-hulterred, and it told the great bay of the wave on the Sy and Beses--thebptilel in the Seme's o'. D\n",
      "==========\n",
      "\u001b[1m7,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m7,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " to the city in the other city and rained on the same carches of the same room. The matter is that he would go, and had been in their own handled, and would have to ask him as he had a week.\n",
      "\n",
      "\n",
      "Sant I told of the great rank\n",
      "==========\n",
      "\u001b[1m8,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m8,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ".\n",
      "\n",
      "I saw in the sea-bargive of the last house, nor was no longer to my own. It was what he wished to do. This was now--fary, and a small hundredteen, and at first he had taken from the house, and did not tell him a\n",
      "==========\n",
      "\u001b[1m9,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m9,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " work, and the terribleness of the old man's beasts in the other, and of the strange-downs in the dark hill and then-down the same handwellers.\n",
      "\n",
      "The Pyonaa, the Marks, and the Gongal Street of the\n",
      "==========\n",
      "\u001b[1m10,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m10,000 steps reached: generating sample texts.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen:Saving trained model pytorch_model.bin to /trained_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "==========\n",
      "\r",
      " in the old and the old man's body before.\n",
      "\n",
      "I was not to see what the whole of the Great world had been in the house, and had not been to see the place of the house was no longer before. It was a long the whole of the Sason Sweston,\n",
      "\r",
      "==========\n"
     ]
    }
   ],
   "source": [
    "ai.train(data, \n",
    "         num_steps=10000,\n",
    "         num_workers = 4,\n",
    "         batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Functions\n",
    "\n",
    "## Parameters\n",
    "\n",
    "**n**: Number of texts generated\n",
    "\n",
    "**max_length**: Maximum length of the generated text (default: 200; for GPT-2, the maximum is 1024.)\n",
    "\n",
    "**prompt**: Prompt that starts the generated text and is included in the generate text. (used to be prefix in previous tools)\n",
    "\n",
    "**temperature**: Controls the \"craziness\" of the text (recommended to keep between 0.7 and 1.0) \n",
    "\n",
    "**top_k**: If nonzero, limits the sampled tokens to the top k values. (40 is recommended)\n",
    "\n",
    "**top_p**: If nonzero, limits the sampled tokens to the cumulative probability (0.9 is recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ai.generate_samples()**: Generates multiple samples at specified temperatures: great for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Temperature: 0.7\n",
      "####################\n",
      "\u001b[1mAncient ruin in Alabama swamp - voodoo.\u001b[0m\n",
      "\n",
      "DATo, the last of what has gone in the Go, and that the Pore Lad.\n",
      "\n",
      "The next I know of the next time, for the old man had seen in the night before\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "\u001b[1mAncient ruin in Alabama swamp - voodoo.\u001b[0m The porned, I heard a very queerly-book of the Chazing-post--sus, the ruder's dort was the souzous, and its page at this\n",
      "####################\n",
      "Temperature: 1.2\n",
      "####################\n",
      "\u001b[1mAncient ruin in Alabama swamp - voodoo.\u001b[0m The lader came from the sea; and Carter had been so very more more than he had no more than he thought to the last. The time was in the night a certain old man, and it had said that there came\n"
     ]
    }
   ],
   "source": [
    "ai.generate_samples(n=1,\n",
    "            prompt=\"Ancient ruin in Alabama swamp - voodoo.\",\n",
    "            top_k=40,\n",
    "            top_p=0.9, \n",
    "            max_length=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ai.generate_one()**: A helper function which generates a single text and returns as a string (good for APIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ancient ruin in Alabama swamp - voodoo. Louns and I was, for, my dreams and the hideous and unladition I knew in the same kind of a long-radent diarom and I wondered that a little one thing. I did not'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai.generate_one(prompt=\"Ancient ruin in Alabama swamp - voodoo.\",\n",
    "            top_k=40,\n",
    "            top_p=0.9, \n",
    "            max_length=1024, \n",
    "            temperature=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ai.generate()**: Generates and prints text to console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAncient ruin in Alabama swamp - voodoo.\u001b[0m Then and over these weed, I felt the whole rows of a cesselaritically at a blacker, but to keep it from the dark house-wodhed door to a cubetinch\n",
      "==========\n",
      "\u001b[1mAncient ruin in Alabama swamp - voodoo.\u001b[0m But when I did not let myself with that old he was at that he knew before it the next day before a man told them, and if they had seen of its lading gently at the great house in his son.\n",
      "==========\n",
      "\u001b[1mAncient ruin in Alabama swamp - voodoo.\u001b[0m Hanll and Rort. It's old Diliscar's Prile wasnoa at that time, who had no good for an in the night in the end, if no longer might have found;\n",
      "==========\n",
      "\u001b[1mAncient ruin in Alabama swamp - voodoo.\u001b[0m They know that he can't keep.\n",
      "\n",
      "Po, he spoke about, for those day-gaunts was very darkly. But I had to say of what I had a moment's face of the whole and mumul\n"
     ]
    }
   ],
   "source": [
    "ai.generate(n=4,\n",
    "            prompt=\"Ancient ruin in Alabama swamp - voodoo.\",\n",
    "            top_k=40,\n",
    "            top_p=0.9, \n",
    "            max_length=1024, \n",
    "            temperature=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ai.generate_to_file()**: Generates a bulk amount of texts to file. (this accepts a batch_size parameter which is useful if using on a GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen:Generating 100 texts to ATG_20200831_025307_32961397.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1341aa3c9e06447bb0c906898c656cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Last digits of saved file are seed number that can be used to reproduce results.\n",
    "\n",
    "ai.generate_to_file(n=100, \n",
    "                    prompt=\"Ancient ruin in Alabama swamp - voodoo.\",\n",
    "                    top_k=40,\n",
    "                    top_p=0.9, \n",
    "                    max_length=1024, \n",
    "                    temperature=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
